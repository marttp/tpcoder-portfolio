---
title: "What I Learned from OpenAI's Harness Engineering: Building Software with 0 Lines of Human-Written Code (Is This the Future?)"
description: "A deep dive into what OpenAI shared about building an entire product using only Codex agents — what broke, what compounded, and what it means for backend engineers thinking about the agent-first future."
pubDate: "2026-02-17"
tags:
  [
    "AI",
    "Software Engineering",
    "Coding Agent",
    "Backend",
    "Architecture",
    "Developer Experience",
  ]
language: "en"
translationSlug: "what-i-learned-from-openai-harness-engineering-agent-first-development-th"
---

import MermaidDiagram from "../../components/blog/MermaidDiagram.astro";

OpenAI recently published a blog post titled ["Harness engineering: leveraging Codex in an agent-first world"](https://openai.com/index/harness-engineering/) and after reading through it, the implications for how backend engineers work are hard to ignore.

The short version: a small team at OpenAI built and shipped an internal product with **zero lines of manually-written code**. Every line — application logic, tests, CI configuration, documentation, observability, internal tooling — was written by Codex agents. They estimate it took roughly **1/10th the time** compared to writing the code by hand.

This isn't a toy demo. The product has daily internal users and external alpha testers. It ships, deploys, breaks, and gets fixed. All by agents.

After reading through the full post, there are patterns here that resonate strongly with how backend systems are designed — and some that challenge conventional engineering norms entirely. Here's a breakdown of what stood out and why it matters.

## The Setup: From Empty Repo to a Million Lines in 5 Months

The team started with a completely empty git repository in late August 2025. The initial scaffold — repo structure, CI config, formatting rules, package manager setup, and the application framework — was generated by Codex CLI using GPT-5. Even the initial `AGENTS.md` file that directs agents how to work in the repository was itself written by Codex.

Five months later:

- **~1,000,000 lines of code** across application logic, infrastructure, tooling, documentation, and internal developer utilities
- **~1,500 pull requests** opened and merged
- Started with **3 engineers**, grew to **7**
- Average throughput: **3.5 PRs per engineer per day**
- Throughput actually **increased** as the team grew (which is counterintuitive — usually more people means more coordination overhead)

The constraint they set for themselves is particularly interesting: **humans never directly contributed any code**. This wasn't accidental. It was a deliberate philosophy to force the team to build the infrastructure that makes agents maximally productive.

## Redefining What "Engineering" Means

This is the part that resonated the most from a backend perspective.

The early progress was slower than expected — not because Codex was incapable, but because **the environment was underspecified**. The agent lacked the tools, abstractions, and internal structure required to make progress toward high-level goals.

This is the same problem backend engineers deal with when onboarding new team members or setting up new microservices. If the environment doesn't have clear boundaries, documented conventions, and the right abstractions, even talented humans struggle. Agents just make that gap more visible and more immediate.

The team's response was depth-first: break down larger goals into smaller building blocks, prompt the agent to construct those blocks, and use them to unlock more complex tasks. When something failed, the question was never "try harder" — it was always:

> "What capability is missing, and how do we make it both legible and enforceable for the agent?"

From a backend engineer's point of view, this is essentially **platform engineering for AI agents**. The same discipline that goes into building internal developer platforms — clear APIs, self-service tooling, documented guardrails — now applies to enabling agent productivity.

<MermaidDiagram
  chart={`
flowchart LR
    A[Human Engineer] -->|Writes prompt| B[Codex Agent]
    B -->|Opens PR| C[Agent Reviewers]
    C -->|Feedback loop| B
    B -->|Escalates when stuck| A
    C -->|All satisfied| D[Merge]
`}
  size="medium"
/>

The human role shifts from writing code to **designing environments, specifying intent, and building feedback loops**. Humans interact with the system almost entirely through prompts. They describe a task, run the agent, and let it open a pull request. Multiple agent reviewers provide feedback in a loop until all are satisfied. Humans may review PRs, but aren't required to.

## Making the Application Legible to Agents

This section is where the backend engineering implications get really concrete.

As code throughput increased, the bottleneck became **human QA capacity**. The team's solution: make the application itself directly legible to Codex.

### Browser Automation for UI Validation

They made the app bootable per git worktree, so Codex could launch and drive one instance per change. They wired the **Chrome DevTools Protocol** into the agent runtime and created skills for working with DOM snapshots, screenshots, and navigation. This enabled Codex to reproduce bugs, validate fixes, and reason about UI behavior directly.

<MermaidDiagram
  chart={`
flowchart TD
    A[Codex selects target] --> B[Snapshot state BEFORE]
    B --> C[Trigger UI path]
    C --> D[Observe runtime events\nvia Chrome DevTools]
    D --> E[Snapshot state AFTER]
    E --> F{Clean?}
    F -->|No| G[Apply fixes & restart]
    G --> A
    F -->|Yes| H[Task complete]
`}
  size="medium"
/>

### Full Observability Stack in Local Dev

This is the part that excited me the most as someone who works with observability daily. They gave Codex a **full local observability stack** — logs, metrics, and traces — that is ephemeral per worktree. Each agent works on a fully isolated version of the app including its own observability data, which gets torn down once the task is complete.

Agents can query logs with **LogQL** and metrics with **PromQL**. With this context available, prompts like these become tractable:

- _"Ensure service startup completes in under 800ms"_
- _"No span in these four critical user journeys exceeds two seconds"_

<MermaidDiagram
  chart={`
flowchart LR
    A[App] -->|Logs, Metrics, Traces| B[Vector]
    B --> C[Victoria Logs\nLogQL]
    B --> D[Victoria Metrics\nPromQL]
    B --> E[Victoria Traces\nTraceQL]
    C & D & E --> F[Codex queries\n& correlates signals]
    F --> G[Implements fixes]
    G --> A
`}
  size="medium"
/>

They regularly see single Codex runs work on a single task for **upwards of six hours** — often while the humans are sleeping. That's a night shift no one has to staff.

From a backend systems perspective, this is essentially **SRE-as-code**: performance budgets, latency SLOs, and startup time requirements encoded as agent-executable prompts rather than dashboards humans stare at.

## Repository Knowledge as the System of Record

This section validates something many backend engineers already know: **documentation architecture matters as much as code architecture**.

### The "One Big AGENTS.md" Anti-Pattern

The team initially tried putting everything into one large `AGENTS.md` file. It failed predictably:

1. **Context is scarce** — a giant instruction file crowds out the actual task, code, and relevant docs
2. **Too much guidance becomes non-guidance** — when everything is "important," nothing is
3. **It rots instantly** — a monolithic manual turns into a graveyard of stale rules
4. **It's hard to verify** — a single blob doesn't lend itself to mechanical checks

### Progressive Disclosure Instead

Instead, they treat `AGENTS.md` as a **table of contents** (~100 lines) that points to deeper sources of truth:

```
AGENTS.md
ARCHITECTURE.md
docs/
├── design-docs/
│   ├── index.md
│   ├── core-beliefs.md
│   └── ...
├── exec-plans/
│   ├── active/
│   ├── completed/
│   └── tech-debt-tracker.md
├── generated/
│   └── db-schema.md
├── product-specs/
│   ├── index.md
│   ├── new-user-onboarding.md
│   └── ...
├── references/
│   ├── design-system-reference-llms.txt
│   ├── nixpacks-llms.txt
│   └── uv-llms.txt
├── DESIGN.md
├── FRONTEND.md
├── PLANS.md
├── PRODUCT_SENSE.md
├── QUALITY_SCORE.md
├── RELIABILITY.md
└── SECURITY.md
```

This enables **progressive disclosure**: agents start with a small, stable entry point and are taught where to look next, rather than being overwhelmed up front.

They enforce this mechanically — dedicated linters and CI jobs validate that the knowledge base is up to date, cross-linked, and structured correctly. A recurring **"doc-gardening" agent** scans for stale documentation and opens fix-up PRs.

The key insight here is powerful:

> From the agent's point of view, anything it can't access in-context while running effectively doesn't exist. Knowledge that lives in Google Docs, chat threads, or people's heads is not accessible. Repository-local, versioned artifacts are all it can see.

That Slack discussion that aligned the team on an architectural pattern? If it isn't committed to the repo, it's illegible to the agent — in the same way it would be unknown to a new hire joining three months later.

This is a pattern that backend teams should adopt regardless of whether they use AI agents. The discipline of making decisions discoverable in-repo is good engineering practice.

## Enforcing Architecture and Taste Mechanically

This resonated with experience building enterprise-scale systems. The team built a **rigid architectural model** — each business domain divided into fixed layers with strictly validated dependency directions.

<MermaidDiagram
  chart={`
flowchart LR
    T[Types] --> Co[Config] --> R[Repo]
    P[Providers] --> S[Service] --> Rt[Runtime] --> U[UI]
    Ut[Utils] --> P
`}
  size="medium"
/>

The rule: within each business domain, code can only depend "forward" through fixed layers (Types → Config → Repo → Service → Runtime → UI). Cross-cutting concerns (auth, connectors, telemetry, feature flags) enter through a single explicit interface: **Providers**.

This is enforced mechanically via custom linters and structural tests — all generated by Codex. The error messages in those lints are deliberately written to inject **remediation instructions** into agent context, so when a lint fails, the agent knows exactly how to fix it.

An important quote from the post:

> This is the kind of architecture you usually postpone until you have hundreds of engineers. With coding agents, it's an early prerequisite: the constraints are what allows speed without decay or architectural drift.

The team also made a pragmatic observation about technology choices: **"boring" technologies** tend to be easier for agents to model due to composability, API stability, and representation in the training set. In some cases, they had the agent **reimplement subsets of functionality** rather than pull in opaque third-party libraries — because a tightly integrated, 100%-tested internal implementation is more legible to future agent runs than a generic npm package with unpredictable behavior.

This aligns with something observable in backend engineering: the projects that scale best are rarely the ones using the newest frameworks. They're the ones with the clearest boundaries and most predictable behavior.

## Throughput Changes the Merge Philosophy

This was the most counterintuitive section. As agent throughput increased, **conventional engineering norms became counterproductive**.

- PRs are short-lived
- Minimal blocking merge gates
- Test flakes addressed with follow-up runs rather than blocking progress indefinitely
- Corrections are cheap; waiting is expensive

The team acknowledges this would be irresponsible in a low-throughput environment. But when your system can produce and fix code faster than humans can review it, the cost calculus changes. The bottleneck is human attention, not code production.

## The Full Autonomy Loop

The most impressive result: given a single prompt, Codex can now end-to-end drive a new feature:

<MermaidDiagram
  chart={`
flowchart TD
    A[Receive prompt] --> B[Validate codebase state]
    B --> C[Reproduce reported bug]
    C --> D[Record video of failure]
    D --> E[Implement fix]
    E --> F[Validate fix by driving app]
    F --> G[Record video of resolution]
    G --> H[Open pull request]
    H --> I[Respond to agent/human feedback]
    I --> J[Detect & fix build failures]
    J --> K{Human judgment needed?}
    K -->|Yes| L[Escalate to human]
    K -->|No| M[Merge]
`}
/>

The team is careful to note this depends heavily on the specific structure and tooling of their repository and should not be assumed to generalize without similar investment.

## Entropy and Garbage Collection: The Honest Part

This was the section that felt most real. Full agent autonomy introduces **entropy**. Codex replicates patterns that already exist — even suboptimal ones. Over time, this leads to drift.

The team initially spent **every Friday (20% of their week)** cleaning up "AI slop." That didn't scale.

Their solution: encode **"golden principles"** into the repository and run a recurring cleanup process. On a regular cadence, background Codex tasks scan for deviations, update quality grades, and open targeted refactoring PRs. Most of these can be reviewed in under a minute and automerged.

They describe this as **garbage collection** for the codebase:

> Technical debt is like a high-interest loan: it's almost always better to pay it down continuously in small increments than to let it compound and tackle it in painful bursts.

This is the most transferable insight in the entire post. Whether the code is agent-generated or human-written, continuous small cleanups beat quarterly "tech debt sprints" every time.

## My Two Cents: What This Means for Backend Engineers

After reading through all of this, a few things stand out:

### 1. Backend Engineers Become Platform Engineers for Agents

The skills that matter in an agent-first world — designing clear APIs, building observability pipelines, enforcing architectural boundaries, creating self-service tooling — are the same skills backend and platform engineers already have. This isn't a pivot; it's an elevation.

### 2. "Boring" Technology Wins (Again)

The observation that agents work better with composable, stable, well-documented technologies validates what experienced backend engineers have known: the best technology choice is often the most predictable one. Spring Boot, PostgreSQL, well-structured REST APIs — these become more valuable, not less, when agents are writing the code.

### 3. Documentation Becomes Infrastructure

The shift from "docs are nice to have" to "docs are the system of record that agents depend on" is significant. If the team's experience is any indication, the engineers who invest in documentation architecture will have outsized leverage in agent-first environments.

### 4. The 20% "AI Slop" Problem is Real

The honest admission that the team spent a full day per week cleaning up agent-generated mess — before automating it away — is the kind of detail that usually gets left out of corporate blog posts. It suggests that any team adopting heavy agent usage should budget for entropy management from day one.

### 5. The Feedback Loop is the Product

The most interesting meta-observation: the product the team actually built wasn't just the internal beta app. It was the **development environment itself** — the linters, the observability stack, the documentation structure, the review loops. The app was almost a side effect of building a system where agents can do reliable work.

## What's Still Unknown (And That's Okay)

The team is honest about what they don't know yet:

- How does architectural coherence evolve over **years** in a fully agent-generated system?
- Where does human judgment add the **most leverage**?
- How does this system evolve as models continue to become **more capable**?

These are the right questions. And the fact that a team at OpenAI — with access to the most capable models — is still asking them suggests that the rest of us have time to learn and adapt rather than panic.

---

## Summary

The OpenAI harness engineering post is one of the most concrete examples yet of what agent-first software development looks like in practice. It's not about removing engineers — it's about changing what engineers do. The shift from "writing code" to "designing environments where agents write code" is real, and the skills that matter most are architecture, observability, documentation, and feedback loop design.

For backend engineers specifically, the takeaway is encouraging: the discipline of building reliable, observable, well-bounded systems is exactly what makes agent-first development possible. The foundation has already been built — it just needs to be pointed in a new direction.

---

**Reference:** [Harness engineering: leveraging Codex in an agent-first world — OpenAI (Feb 13, 2026)](https://openai.com/index/harness-engineering/)
